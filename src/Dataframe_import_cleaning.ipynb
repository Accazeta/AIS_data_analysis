{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p              # pandas\n",
    "import os                       # for reading local files and folders\n",
    "import zipfile                  # for handling zip files\n",
    "from collections import Counter # for stats analysis on the dataframe  \n",
    "import math                     # for math functions\n",
    "import matplotlib.pyplot as plt #\n",
    "import statistics as stats      #\n",
    "import folium                   # map\n",
    "\n",
    "# constants\n",
    "FOLDER_PATH = \"../dataset/2019_01/\"\n",
    "LAT_MIN = 18.76651\n",
    "LAT_MAX = 22.63089\n",
    "LON_MIN = -160.11085\n",
    "LON_MAX = -154.38957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only the data with the correct latitude and longitude values</br>\n",
    "This operation takes roughly 15-20 seconds per csv file (tested on csv files with > 1 million rows)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = p.DataFrame()\n",
    "\n",
    "# list all the files in the specified directory\n",
    "filenames = os.listdir(FOLDER_PATH)\n",
    "# and sort them alphabetically\n",
    "filenames.sort()\n",
    "# for each file\n",
    "for file_name in filenames:\n",
    "    # only iterate over zip files\n",
    "    if file_name.endswith('.zip'):\n",
    "        file_path = os.path.join(FOLDER_PATH, file_name)\n",
    "        # if the csv file has yet to be extracted\n",
    "        csv_filename = file_name.replace(\".zip\", \".csv\")\n",
    "        if not(os.path.exists(os.path.join(FOLDER_PATH, csv_filename))):\n",
    "            # open the zip file and extract it\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "                csv_filename = zip_file.namelist()[0]\n",
    "                zip_file.extractall(FOLDER_PATH)\n",
    "        # create a dataframe using the extracted csv file\n",
    "        full_df = p.read_csv(os.path.join(FOLDER_PATH, csv_filename))\n",
    "        # extract only the rows of interest (with LAT and LON values between the specified bounds)\n",
    "        filtered_df = full_df[\n",
    "                                (full_df['LAT'].between(LAT_MIN, LAT_MAX)) &\n",
    "                                (full_df['LON'].between(LON_MIN, LON_MAX)) \n",
    "                            ]\n",
    "        # and append the extracted rows to the final dataframe\n",
    "        final_df = p.concat([final_df, filtered_df], ignore_index = True)\n",
    "\n",
    "print(f\"Final dataframe len: {final_df.shape[0]}\")\n",
    "print(final_df.head())\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate code snippet used for exporting the final dataframe as a csv file.</br>\n",
    "This is necessary as the above code snippet takes ~8 minutes read and filter 31 csv files.</br>\n",
    "By exporting the resulting df as a csv file it's possible import it again for the following part of this notebook.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = os.getcwd() # returns current workink dir\n",
    "OUTPUT_PATH = FOLDER_PATH\n",
    "FILE_NAME = '2019_01.csv' #specify file name (.csv extension necessary)\n",
    "\n",
    "# export the full, uncleaned csv for backup purposes\n",
    "final_df.to_csv(OUTPUT_PATH + '/' + FILE_NAME, index = False)\n",
    "\n",
    "print(f\"Dataframe shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file and use it as the final data source</br>\n",
    "(The code snippet below should be used only if the code snippet #2 was not executed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "final_df = p.read_csv(OUTPUT_PATH + FILE_NAME)\n",
    "final_df.drop_duplicates(keep='first', inplace=True, ignore_index=True) # drop duplicates\n",
    "final_df.BaseDateTime = p.to_datetime(final_df.BaseDateTime) # Convert DateTime into more readable format\n",
    "\n",
    "print(f\"Dataframe shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stats about the imported dataframe</br>\n",
    "In order to make the histogram below, occurrencies are rounded to the top hundred (ex: 33 -> 100, 244 -> 300, etc...)</br>\n",
    "This is done in order to have an idea about how many ships have negligible amount of records.</br>\n",
    "In the next code snippet (#10) the threshold is set to 100 and consequently all vessels with less than 100 data points are removed from the dataset</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entries (number of rows of the dataframe)\n",
    "num_of_rows = final_df.index.shape[0]\n",
    "print(\"Num of rows: \" + str(num_of_rows))\n",
    "\n",
    "# Extract all the unique MMSIs\n",
    "unique_vessels_ids = p.unique(final_df['MMSI']).tolist()\n",
    "num_of_vessels = len(unique_vessels_ids)\n",
    "print(\"Num of vessels: \" + str(num_of_vessels))\n",
    "\n",
    "# Calculate the exact number of entries for each ship\n",
    "vessels_entries_dict = dict(Counter(final_df['MMSI'].to_list())) # save it in a dict\n",
    "vessels_entries = list(vessels_entries_dict.values()) # save just the values in a list\n",
    "vessels_entries.sort()\n",
    "rounded_vessels_entries = [math.ceil(x/100) * 100 for x in vessels_entries]\n",
    "\n",
    "print(f\"Avg entries for each vessel {stats.mean(vessels_entries)}\")\n",
    "print(f\"Variance: {stats.variance(vessels_entries)}\")\n",
    "print(f\"Standard Deviation: {stats.stdev(vessels_entries)}\")\n",
    "print(f\"Max entries found for the same ship: {vessels_entries[-1]}\")\n",
    "print(f\"Min entries found for the same ship: {vessels_entries[0]}\")\n",
    "\n",
    "n, bins, patches = plt.hist(x=rounded_vessels_entries, bins='auto', color='orange', edgecolor = 'black')\n",
    "\n",
    "\n",
    "print(f\"Final dataframe len: {final_df.shape[0]}, num of vessels: {len(vessels_entries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove ships with not enough points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 100\n",
    "\n",
    "mmsi_to_be_removed = [x for x in vessels_entries_dict.keys() if vessels_entries_dict[x] <= THRESHOLD]\n",
    "print(f\"Final dataframe len: {final_df.shape[0]}, num of vessels: {len(vessels_entries)}, unwanted vessels: {len(mmsi_to_be_removed)}\")\n",
    "clean_df = final_df.copy()\n",
    "\n",
    "for unwanted_mmsi in mmsi_to_be_removed:\n",
    "    clean_df = clean_df.drop(clean_df[clean_df['MMSI'] == unwanted_mmsi].index)\n",
    "\n",
    "clean_df.sort_values(by='BaseDateTime', inplace=True, ignore_index=True)\n",
    "\n",
    "print(f\"Cleaned dataframe len: {clean_df.shape[0]}, num of vessels: {len(p.unique(clean_df['MMSI']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the cleaned dataframe in order to have a backup copy in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = os.getcwd()\n",
    "OUTPUT_PATH = FOLDER_PATH\n",
    "FILE_NAME = 'cleaned_2019_01.csv' #specify file name (don't forget the .csv extension)\n",
    "clean_df.to_csv(OUTPUT_PATH + '/' + FILE_NAME, index = False)\n",
    "\n",
    "print(f\"Dataframe shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new histogram using the \"cleaned\" data from the clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vessel_entries = list(Counter(clean_df['MMSI'].to_list()).values())\n",
    "\n",
    "rounded_clean_vessels_entries = [math.ceil(x/100) * 100 for x in clean_vessel_entries]\n",
    "\n",
    "print(f\"Avg entries for each vessel {stats.mean(clean_vessel_entries)}\")\n",
    "print(f\"Variance: {stats.variance(clean_vessel_entries)}\")\n",
    "print(f\"Standard Deviation: {stats.stdev(clean_vessel_entries)}\")\n",
    "print(f\"Max entries found for the same ship: {max(clean_vessel_entries)}\")\n",
    "print(f\"Min entries found for the same ship: {min(clean_vessel_entries)}\")\n",
    "\n",
    "n, bins, patches = plt.hist(x=rounded_clean_vessels_entries, bins='auto', color='green', edgecolor = 'black', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a few points on the map just to check that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = folium.Map(location=[21, -158], tiles=\"OpenStreetMap\", zoom_start=9)\n",
    "\n",
    "clean_df.sort_values(by='MMSI', inplace=True)\n",
    "\n",
    "#test ranges (replace x and y below): [2:474], [474, 1364], [1364,1837], [1837:2285], [2285, 2725]\n",
    "x = 2\n",
    "y = 30000\n",
    "# extract from the dataset latitude and longitude of the values in range [x:y)\n",
    "loc = [[str(mmsi), lat, lon] for (mmsi, lat, lon) in zip(clean_df.iloc[x:y]['MMSI'], clean_df.iloc[x:y]['LAT'], clean_df.iloc[x:y]['LON'])]\n",
    "\n",
    "appendDF = p.DataFrame(loc)\n",
    "appendDF.columns = ['MMSI', 'LAT', 'LON'] # create a temporary df with the extracted mmsi, lat. and long. values\n",
    "\n",
    "# CircleMarker requires each point to be added one by one\n",
    "# The temp function below is applied to all points in the tempDF\n",
    "def tempFunc(row):\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LAT'],row['LON']],\n",
    "        radius=0.5\n",
    "        ).add_child(folium.Popup(row['MMSI'])).add_to(map)\n",
    "\n",
    "# Using 'apply' with a custom function saves a bit of time (compared to a simple for-loop)\n",
    "appendDF.apply(tempFunc, axis=1)\n",
    "\n",
    "map "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
